---
title: "Project 3: Main Script"
author: "Group 2"
date: "March 24, 2017"
output:
  pdf_document: default
  html_document: default
---

This file firstly details the model selection process for an advanced binary classification model, that takes in images and classifies them as either a labradoodle ("1") or fried chicken ("0") - simply referred to as the Advanced Model. Secondly, it runs evaluation experiments to compare the Baseline Model - gradient boosting model (gbm) with parameters tuned on the original SIFT features - with our Advanced Model.

```{r} 
#Load (and install if necessary) required packages
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}

if(!require("gbm")){
  install.packages("gbm")
}

library(EBImage)
library(gbm)
library(data.table)
```

### Specify directories.

Set the working directory to the image folder. Specify the training and the testing set. For data without an independent test/validation set, you need to create your own testing data by random subsampling. In order to obain reproducible results, set.seed() whenever randomization is used. 

Provide directories for raw images. Training set and test set should be in different subfolders. 
```{r}
experiment_dir <- "../data/" # This will be modified for different data sets.
img_train_dir <- paste(experiment_dir, "train/", sep="")
img_test_dir <- paste(experiment_dir, "test/", sep="")
```

### Import class labels for training images  
We code labradoodles as "1" and fried chicken as "0" for binary classification.

```{r training_images}
label_train <- read.csv("../data/train/labels.csv")
label_train <- as.matrix(label_train)[,1]

```

##Selecting the Advanced Model: 

### Model selection with provided SIFT features of training images  
Import the SIFT features of the set of 2000 training images
```{r}
train_sift <- read.csv("../data/train/sift_features.csv")
train_sift <- t(as.matrix(train_sift))
```

#### Tune parameters of each model via cross-validation  
```{r}
#get summary tables of best tuned parameters for each model considered, i.e. lowest cv error
#6 columns: "Model", "Best_Param_1", "Best_Param_2", "Best_Param_3", 
#            "Best_Error", "Training_Time"
#  - NA values for Best_Param_2 and/or Best_Param_3 if model has less than 3 params
############################################################################################
###### Uncomment below code to reproduce the tuning process for all models considered ######

#source("../lib/xgboost.R")
#source("../lib/svm.R")
#source("../lib/knn.R")
#source("../lib/AdaBag_AdaBoost.R")

#summary.xgb <- tune.xgb(train_sift,label_train)
#save(summary.xgb, file="output/summary_best_xgb.RData")

#summary.svm.lin <- tune.svm.lin(train_sift,label_train)
#save(summary.svm.lin, file="output/summary_best_svm_lin.Rdata")

#summary.svm.rad <- tune.svm.rad(train_sift,label_train)
#save(summary.svm.rad,file="output/summary_best_svm_rad.Rdata")

#summary.knn <- tune.knn(train_sift,label_train)
#save(summary.knn, file="output/summary_best_knn.Rdata")

#summary.AdaBag <- tune.AdaBag(train_sift,label_train)
#save(summary.AdaBag,file="output/summary_best_AdaBag2.Rdata")

#summary.AdaBoost.M1 <- tune.AdaBoost.M1(train_sift,label_train)
#save(summary.AdaBoost.M1,file="output/summary_best_AdaBoost.M1.Rdata")

#summary.AdaBoost_SAMME <- tune.AdaBoost_SAMME(train_sift,label_train)
#save(summary.AdaBoost_SAMME,file="output/summary_best_AdaBoost_SAMME.Rdata")

```

####Summary Table: parameter-tuning process on original SIFT features
Summary table of *best* models (i.e. models with *best* parameters) tuned on original SIFT features of the training set via cross-validation, sorted by CV Error.  
```{r}
library(data.table)
#load already produced summary tables for best tuned parameters for each model considered
load("../output/summary_best_xgb.RData")
load("../output/summary_best_svm_lin.Rdata")
load("../output/summary_best_svm_rad.Rdata")
load("../output/summary_best_knn.Rdata")

#here we have two separate tables for the AdaBag algorithm
#   -tuning with cross validation chooses the second one (mfinal=100) 
#   -factoring in training time, the first one (mfinal=10), is better
load("../output/summary_best_AdaBag.Rdata") #mfinal=10; higher cv error, but smaller training time
load("../output/summary_best_AdaBag2.Rdata") #mfinal=100; lower cv error, but larger training time

load("../output/summary_best_AdaBoost.M1.Rdata")
load("../output/summary_best_AdaBoost_SAMME.Rdata")

#6 columns: "Model", "Best_Param_1", "Best_Param_2", "Best_Param_3", 
#            "Best_Error", "Training_Time"
#  - NA values for Best_Param_2 and/or Best_Param_3 if model has less than 3 params
summary1 <- rbind(summary.xgb, summary.svm.lin,
                 summary.svm.rad, summary.AdaBag,
                 summary.AdaBag2, summary.AdaBoost.M1,
                 summary.AdaBoost_SAMME,
                 summary.knn) 
#sort table by Best_Error in ascending order
summary1 <- summary1[order(summary$Best_Error)]

#add column for feature set models are tuned on
(summary1 <- data.table(summary1[,1], Features = rep("Original SIFT",8), summary1[,2:6]))

save(summary1,file="../output/summary_best_models1.Rdata")

```


### Model selection with new visual features  
####Import sets of new visual features  
```{r}
#import the three csv files - the three new sets of visual features
new_train_feat1 <- read.csv("../data/train/sift_features_resize+adaptive.csv")
new_train_feat1 <- t(as.matrix(new_train_feat1))

new_train_feat2 <- 
new_train_feat3 <- 
```

#### Tune parameters of each model via cross-validation 
```{r}
##NOTE: Uncomment the specified chunks of code to reproduce the parameter-tuning process
##      for the xgBoost model on the three new feature set and the gbm on the three new feature sets
#########============Uncomment Code Below============#########
#source("../lib/xgboost.R")
#source("../lib/bl.tune.R")
#########============================================#########

###******* Tune for xgBoost
#  1)tune on the first set of new visual features for the training images
#########============Uncomment Code Below============#########
#summary.xgb.new1 <- tune.xgb(new_train_feat1,label_train)
#save(summary.xgb.new1,file="../output/summary_best_xgb_new1.Rdata")
#########============================================#########
#  2)tune on the second set of new visual features for the training images
#########============Uncomment Code Below============#########
summary.xgb.new2 <- tune.xgb(new_train_feat2,label_train)
save(summary.xgb.new2,file="../output/summary_best_xgb_new2.Rdata")
#########============================================#########
#  3)tune on the third set of new visual features for the training images
#########============Uncomment Code Below============#########
summary.xgb.new3 <- tune.xgb(new_train_feat3,label_train)
save(summary.xgb.new3,file="../output/summary_best_xgb_new3.Rdata")
#########============================================#########

###******* Tune for gbm 
#  1)tune on the first set of new visual features for the training images
#########============Uncomment Code Below============#########
summary.gbm.new1 <- bl.tune()
save(summary.gbm.new1,file="../output/summary_best_gbm_new1.Rdata")
#########============================================#########
#  2)tune on the second set of new visual features for the training images
#########============Uncomment Code Below============#########
summary.gbm.new2 <- bl.tune()
save(summary.gbm.new2,file="../output/summary_best_gbm_new2.Rdata")
#########============================================#########
#  3)tune on the third set of new visual features for the training images
#########============Uncomment Code Below============#########
summary.gbm.new3 <- bl.tune()
save(summary.gbm.new3,file="../output/summary_best_gbm_new3.Rdata")
#########============================================#########
```


####Summary Table: parameter-tuning process on new features 
```{r}
#load Rdata files
load("../output/summary_best_models1.Rdata")

data.table(Model = ,
           Features = ,
           Best_Param_1 = ,
           Best_Param_2 = ,
           Best_Param_3 = ,
           Best_CV_Error = ,
           Training_Time = )
```


###Selecting the *best* Advanced Model  
####Summary Table of best models tuned on various feature sets (original & new) 
```{r}
load("../output/summary_best_models1.Rdata")
```


##Comparing the Baseline Model and the Advanced Model  
### Set up controls for evaluation experiments: Baseline Model vs. Advanced Model

+ (T/F) train Baseline Model and Advanced Model
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set for both Baseline Model and Advanced Model
+ (T/F) process features for test set

```{r exp_setup}
run.train=TRUE # train 'best' model 
run.feature.train=TRUE # process features for training set
run.test=FALSE # run evaluation on an independent test set
run.feature.test=FALSE # process features for test set
```


### Construct the visual features  
These features are to be used in training and testing of our Baseline and Advanced model. Save the constructed features to the output subfolder.

`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features that are required by all the models you are going to evaluate later. 

```{r feature}
source("../lib/feature.R")

tm_feature_train <- NA
if(run.feature.train){
  tm_feature_train <- system.time(dat_train <- feature(img_train_dir, 
                                                       "train", 
                                                       data_name="zip", 
                                                       export=TRUE))
}

tm_feature_test <- NA
if(run.feature.test){
  tm_feature_test <- system.time(dat_test <- feature(img_test_dir, 
                                                     "test", 
                                                     data_name="zip", 
                                                     export=TRUE))
}

#save(dat_train, file="./output/feature_train.RData")
#save(dat_test, file="./output/feature_test.RData")
```


### Train a classification model with training images
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 
+ `train.R`
  + Input: an R object that contains processed training set features.
  + Input: an R object of training sample labels.
  + Output: an RData file that contains trained classifiers in the forms of R objects: models/settings/links to external trained configurations.
+ `test.R`
  + Input: a path that points to the test set features.
  + Input: an R object that contains a trained classifiers.
  + Output: an R object of class label predictions on the test set. If there are multiple classifiers under evaluation, there should be multiple sets of label predictions. 
```{r loadlib}
source("../lib/train.R")
source("../lib/test.R")
```


* Train the baseline and advanced models with the entire training set using the selected models ('best' model parameters) via cross-validation.
```{r final_train}
#Here, we call train() from train.R
#   -note that train() returns a list of two model objects: list(baseline_fit,advanced_fit)
tm_train=NA
if(run.train){
  load(file="../output/feature_train.RData")
  tm_train <- system.time(fit_train <- train(dat_train, label_train))
  save(fit_train, file="../output/fit_train.RData")
}
```

### Make prediction 
Feed the final training model with the completely holdout testing data. 
```{r test}
tm_test=NA
if(run.test){
  load(file="../output/feature_test.RData")
  load(file="../output/fit_train.RData")
  tm_test <- system.time(pred_test <- test(fit_train, dat_test))
  save(pred_test, file="../output/pred_test.RData")
}

```

### Summarize Running Time
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for making prediction=", tm_test[1], "s \n")
```
