---
title: "Project 3: Main Script"
author: "Group 2"
date: "March 24, 2017"
output:
  pdf_document: default
  html_document: default
---

This file firstly details the model selection process for an advanced binary classification model, that takes in images and classifies them as either a labradoodle ("1") or fried chicken ("0") - simply referred to as the Advanced Model. Secondly, it runs evaluation experiments to compare the Baseline Model - gradient boosting model (gbm) with parameters tuned on the original SIFT features - with our Advanced Model.

```{r} 
#Load (and install if necessary) required packages
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}


needed <- setdiff(c("gbm", "data.table", "e1071", "class", "adabag"),rownames(installed.packages()))
install.packages(needed)

library(EBImage)
library(gbm)
library(data.table)
library(e1071)
library(class)
library(adabag)
```

### Specify directories.

Set the working directory to the image folder. Specify the training and the testing set. For data without an independent test/validation set, you need to create your own testing data by random subsampling. In order to obain reproducible results, set.seed() whenever randomization is used. 

Provide directories for raw images. Training set and test set should be in different subfolders. 
```{r}
experiment_dir <- "../data/" # This will be modified for different data sets.
img_train_dir <- paste(experiment_dir, "train/", sep="")
img_test_dir <- paste(experiment_dir, "test/", sep="")
```

### Import class labels for training images  
We code labradoodles as "1" and fried chicken as "0" for binary classification.

```{r training_images}
label_train <- read.csv("../data/train/labels.csv")
label_train <- as.matrix(label_train)[,1]  
#some of our models require labels to be numeric in [0,1], so we keep the labels as 0's and 1's
#for any model that requires labels to be factors we pass the labels to the tune or train function
#specific to the model as numeric values and coerce them inside the function to be factors
```

##Selecting the Advanced Model: 

### Model selection with provided SIFT features of training images  
Import the SIFT features of the set of 2000 training images
```{r}
train_sift <- read.csv("../data/train/sift_features.csv")
train_sift <- t(as.matrix(train_sift))
```

#### Tune parameters of each model via cross-validation  
Set up control for tuning models under consideration based on the provided SIFT features of the training images.  
+ (T/F) train Baseline Model and Advanced Model
```{r}
###### T/F to reproduce the tuning process for all models considered ######
run.tune.sift = FALSE
```


```{r}
#get summary tables of best tuned parameters for each model considered, i.e. lowest cv error
#6 columns: "Model", "Best_Param_1", "Best_Param_2", "Best_Param_3", 
#            "Best_Error", "Training_Time"
#  - NA values for Best_Param_2 and/or Best_Param_3 if model has less than 3 params
############################################################################################

if(run.tune.sift) {
  source("../lib/xgboost.R")
  source("../lib/svm.R")
  source("../lib/knn.R")
  source("../lib/AdaBag_AdaBoost.R")

  summary.xgb <- tune.xgb(train_sift,label_train)
  #save(summary.xgb, file="output/summary_best_xgb.RData")

  summary.svm.lin <- tune.svm.lin(train_sift,label_train)
  #save(summary.svm.lin, file="output/summary_best_svm_lin.Rdata")

  summary.svm.rad <- tune.svm.rad(train_sift,label_train)
  #save(summary.svm.rad,file="output/summary_best_svm_rad.Rdata")

  summary.knn <- tune.knn(train_sift,label_train)
  #save(summary.knn, file="output/summary_best_knn.Rdata")

  summary.AdaBag <- tune.AdaBag(train_sift,label_train)
  #save(summary.AdaBag,file="output/summary_best_AdaBag2.Rdata")

  summary.AdaBoost.M1 <- tune.AdaBoost.M1(train_sift,label_train)
  #save(summary.AdaBoost.M1,file="output/summary_best_AdaBoost.M1.Rdata")

  summary.AdaBoost_SAMME <- tune.AdaBoost_SAMME(train_sift,label_train)
  #save(summary.AdaBoost_SAMME,file="output/summary_best_AdaBoost_SAMME.Rdata")
}

```

####Summary Table: parameter-tuning process on original SIFT features
Summary table of *best* models (i.e. models with *best* parameters) tuned on original SIFT features of the training set via cross-validation, sorted by CV Error.  
```{r}
library(data.table)
#load already produced summary tables for best tuned parameters for each model considered
load("../output/summary_best_xgb.RData")
load("../output/summary_best_svm_lin.Rdata")
load("../output/summary_best_svm_rad.Rdata")
load("../output/summary_best_knn.Rdata")

#here we have two separate tables for the AdaBag algorithm
#   -tuning with cross validation chooses the second one (mfinal=100) 
#   -factoring in training time, the first one (mfinal=10), is better
load("../output/summary_best_AdaBag.Rdata") #mfinal=10; higher cv error, but smaller training time
load("../output/summary_best_AdaBag2.Rdata") #mfinal=100; lower cv error, but larger training time

load("../output/summary_best_AdaBoost.M1.Rdata")
load("../output/summary_best_AdaBoost_SAMME.Rdata")

#6 columns: "Model", "Best_Param_1", "Best_Param_2", "Best_Param_3", 
#            "Best_Error", "Training_Time"
#  - NA values for Best_Param_2 and/or Best_Param_3 if model has less than 3 params
summary1 <- rbind(summary.xgb, summary.svm.lin,
                 summary.svm.rad, summary.AdaBag,
                 summary.AdaBag2, summary.AdaBoost.M1,
                 summary.AdaBoost_SAMME,
                 summary.knn) 
#sort table by Best_Error in ascending order
summary1 <- summary1[order(summary$Best_Error)]

#add column for feature set models are tuned on
(summary1 <- data.table(summary1[,1], Features = rep("Original SIFT",8), summary1[,2:6]))

save(summary1,file="../output/summary_best_models1.Rdata")

```


### Model selection with new visual features  
####Import sets of new visual features  
```{r}
#import the three csv files - the three new sets of visual features
new_train_feat1 <- read.csv("../data/train/sift_features_resize+adaptive.csv")
new_train_feat1 <- t(as.matrix(new_train_feat1))

new_train_feat2 <- 
new_train_feat3 <- 
```

#### Tune parameters of each model via cross-validation 
```{r}
##NOTE: Uncomment the specified chunks of code to reproduce the parameter-tuning process
##      for the xgBoost model on the three new feature set and the gbm on the three new feature sets

if (run.tune.new)
  source("../lib/xgboost.R")
  source("../lib/bl.tune.R")


###******* Tune for xgBoost
#  1)tune on the first set of new visual features for the training images
#########============Uncomment Code Below============#########
#summary.xgb.new1 <- tune.xgb(new_train_feat1,label_train)
#save(summary.xgb.new1,file="../output/summary_best_xgb_new1.Rdata")
#########============================================#########
#  2)tune on the second set of new visual features for the training images
#########============Uncomment Code Below============#########
summary.xgb.new2 <- tune.xgb(new_train_feat2,label_train)
save(summary.xgb.new2,file="../output/summary_best_xgb_new2.Rdata")
#########============================================#########
#  3)tune on the third set of new visual features for the training images
#########============Uncomment Code Below============#########
summary.xgb.new3 <- tune.xgb(new_train_feat3,label_train)
save(summary.xgb.new3,file="../output/summary_best_xgb_new3.Rdata")
#########============================================#########

###******* Tune for gbm 
#  1)tune on the first set of new visual features for the training images
#########============Uncomment Code Below============#########
summary.gbm.new1 <- bl.tune()
save(summary.gbm.new1,file="../output/summary_best_gbm_new1.Rdata")
#########============================================#########
#  2)tune on the second set of new visual features for the training images
#########============Uncomment Code Below============#########
summary.gbm.new2 <- bl.tune()
save(summary.gbm.new2,file="../output/summary_best_gbm_new2.Rdata")
#########============================================#########
#  3)tune on the third set of new visual features for the training images
#########============Uncomment Code Below============#########
summary.gbm.new3 <- bl.tune()
save(summary.gbm.new3,file="../output/summary_best_gbm_new3.Rdata")
#########============================================#########
```


####Summary Table: parameter-tuning process on new features 
```{r}
#load Rdata files
load("../output/summary_best_xgb_new1.Rdata")
load("../output/summary_best_xgb_new2.Rdata")
load("../output/summary_best_xgb_new3.Rdata")
load("../output/summary_best_gbm_new1.Rdata")
load("../output/summary_best_gbm_new2.Rdata")
load("../output/summary_best_gbm_new3.Rdata")

data.table(Model = ,
           Features = ,
           Best_Param_1 = ,
           Best_Param_2 = ,
           Best_Param_3 = ,
           Best_CV_Error = ,
           Training_Time = )
```


###Selecting the *best* Advanced Model  
####Summary Table of best models tuned on various feature sets (original & new) 
```{r}
#load Rdata files 
load("../output/summary_best_models1.Rdata")

```


##Comparing the Baseline Model and the Advanced Model  
### Set up controls for evaluation experiments: Baseline Model vs. Advanced Model

+ (T/F) train Baseline Model and Advanced Model
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set for both Baseline Model and Advanced Model
+ (T/F) process features for test set

```{r exp_setup}
run.train=TRUE # train 'best' model 
run.feature.train=TRUE # process features for training set
run.test=FALSE # run evaluation on an independent test set
run.feature.test=FALSE # process features for test set
```

### Import given SIFT features new visual features of the test set  
We construct the new visual features outside of R/Rstudio, so we just import the .csv files with the new feature data.
```{r}
#Recall for the training set we imported the .csv files before. So we have:
dat_train_sift <- train_sift
dat_train_new <- new_train_feat1  #or new_train_feat2 or new_train_feat3
if (run.feature.train){
  dat_train_sift <- read.csv("../data/train/sift_features_train.csv")
  dat_train_sift <- t(as.matrix(dat_train_sift))
  
  dat_train_new <- read.csv("../data/train/new_features_train.csv")
  dat_train_new <- t(as.matrix(dat_train_new))
}

##Now for the test set: 
if (run.feature.test){
  #Import the provide SIFT features of the test set
  dat_test_sift <- read.csv("../data/test/sift_features_test.csv")
  dat_test_sift <- t(as.matrix(dat_test_sift))
  #Import the set of new visual features (constructed outside of R)
  dat_test_new <- read.csv("../data/test/new_features_test.csv")  #or other file name
  dat_test_new <- t(as.matrix(dat_test_new))
}

#tm_feature_train = NA
#tm_feature_test = NA
```

### Train a classification model with training images
`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 
+ `train.R`
  + Input: an R object that contains processed training set features.
  + Input: an R object of training sample labels.
  + Output: an RData file that contains trained classifiers in the forms of R objects: models/settings/links to external trained configurations.
+ `test.R`
  + Input: a path that points to the test set features.
  + Input: an R object that contains a trained classifiers.
  + Output: an R object of class label predictions on the test set. If there are multiple classifiers under evaluation, there should be multiple sets of label predictions. 
```{r loadlib}
source("../lib/train.R")
source("../lib/test.R")
```


### Train Baseline Model and Advanced Model  
Here we call the ```train()``` function in ```train.R```. We fit the entire set of constructed visual features of the training images to the Baseline Model and the Advanced Model.  
```{r final_train}
#Here, we call train() from train.R
#   -note that train() returns a list of two model objects: list(baseline_fit,advanced_fit)
tm_train=NA
if(run.train){
  load(file="../output/feature_train.RData")
  tm_train <- system.time(fit_train <- train(dat_train_sift, dat_train_new, label_train))
  save(fit_train, file="../output/fit_train.RData")
}
```

### Make predictions  
Here we call the ```test()``` function in ```test.R```. Feed the Baseline Model and the Advanced Model with the completely holdout testing data.  
```{r test}
tm_test=NA
if(run.test){
  load(file="../output/feature_test.RData")
  load(file="../output/fit_train.RData")
  tm_test <- system.time(pred_test <- test(fit_train, dat_test))
  save(pred_test, file="../output/pred_test.RData")
}

```

### Summarize Running Time
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training models (Baseline + Advanced)=", tm_train[1], "s \n")
cat("Time for making predictions (Baseline + Advanced)=", tm_test[1], "s \n")
```
