{
    "collab_server" : "",
    "contents" : "### Author: Ka Heng (Helen) Lo\n### Project 3\n### ADS Spring 2017\n#############================  Description  ================#############\n##This file contains two functions:\n#   1) Function to tune parameters of xgboost model with input set of features \n#      of training images\n#   2) Function to train data to fit xgboost model given the 'best' params from \n#      tuning via cross-validation \n#########################################################################\n\nrequire(xgboost)\nrequire(data.table)\n\n#train_sift <- read.csv(\"data/train/sift_features.csv\")\n#train_sift <- t(as.matrix(train_sift))\n#train_labels <- read.csv(\"data/train/labels.csv\")\n#train_labels <- as.matrix(train_labels)[,1]\n\n#create xgb.DMatrix object for input (recommended)\n#Dtrain_sift <- xgb.DMatrix(data=train_sift,label=train_labels)\n#dim(Dtrain_sift)\n\n##############################################################################\n\n#Call this function to reproduce process of tuning params for xgb model via CV\n# -- Returns a summary data.table object that details best parameters used \n#    based on training images and also saves the object in a .RData file \ntune.xgb <- function(dat_train,label_train){\n  #create xgb.DMatrix object for input (recommended)\n  Dtrain_sift <- xgb.DMatrix(data=dat_train,label=label_train)\n  #set empty variables \n  best_params <- list() ; best_err <- Inf \n  best_nrounds <- NULL\n  \n  #cv.errors <- data.table(m1 = I(list()), m2 = I(list()), m3 = I(list()),\n  #                        m4 = I(list()), m5 = I(list()))\n  \n  \n  #nthread = number of parallel threads to use\n  \n  ##Control Overfitting:\n  #Control model complexity\n  #max_depth: [1,inf]\n    #maximum depth of a tree; increasing this value makes model more complex\n  #Add randomness to make training robust to noise \n  #eta: [0,1] #step size shrikage; makes boosting process more conservative \n  \n  #vector of different max depth values of the tree (default = 6)\n  depth_vals <- 4:8\n  #vector of different eta values to control learning rate (default = 0.3) \n  eta_vals <- seq(.1,.5,.1)\n  #for reproducible results \n  set.seed(77)\n  for (i in 1:5){\n    for (j in 1:5){\n      my.params <- list(max_depth = depth_vals[i], eta = eta_vals[j])\n      \n      cv.output <- xgb.cv(data=Dtrain_sift,params=my.params, nrounds = 500, \n                          nfold = 5, nthread = 2, early_stopping_rounds = 7, \n                          verbose = 0, maximize = F, prediction = T,\n                          objective = \"binary:logistic\")\n      \n      best_iter <- cv.output$best_iteration\n      min_err <- min(cv.output$evaluation_log$test_error_mean)\n      \n      #update the \"best_ \" variables with value corresponding to min cv error\n      if (min_err < best_err){\n        best_params <- my.params ; best_err <- min_err\n        best_nrounds <- best_iter\n      }\n      #update data.table of cv.errors \n      #cv.errors[[i,j]] = list(min_err,best_iter)\n    }\n  }\n\n  #train best model\n  run.time <- system.time(m <- xgb.train(data=Dtrain_sift, params=best_params, \n                             nrounds=best_nrounds, nthread = 2, \n                             objective = \"binary:logistic\"))\n  run.time <- round(run.time[1],3)\n  cat(\"Time for training model: \" ,run.time, \"s \\n\")\n\n  summary.xgb <- data.table(Model = \"XGBoost\",\n             Best_Param_1 = paste(\"max_depth =\",best_params[[1]]),\n             Best_Param_2 = paste(\"eta =\", best_params[[2]]),\n             Best_Param_3 = paste(\"nrounds =\", best_nrounds),\n             Best_Error = best_err,\n             Training_Time = paste(run.time, \"s\"))\n  #save(summary.xgb, file=\"output/summary_best_xgb.RData\")\n  return(summary.xgb)\n}\n#tune.xgb()\n\n\n###############################################################################\n\n##if it's decided that xgboost is best candidate for advanced model\n##then source this R file and call this function in train.R\ntrain.xgb <- function(dat_train,label_train,par_list,nrounds){\n  #use parameters from tuning with CV on the sift features of training images \n  #best_params <- list(max_depth=5, eta=.5) ; nrounds=169 \n  label_train <- as.numeric(label_train) #label vals in range [0,1] for logistic reg.\n  Dtrain <- xgb.DMatrix(data=dat_train,label=label_train)\n  xgb.m <- xgb.train(data=Dtrain, params=par_list, \n                     nthread = 2, nrounds = nrounds,\n                     objective = \"binary:logistic\")\n  return(xgb.m)\n}",
    "created" : 1490132971645.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1924051930",
    "id" : "193E7BB1",
    "lastKnownWriteTime" : 1490158454,
    "last_content_update" : 1490158454770,
    "path" : "~/Desktop/Applied Data Science/Proj3/spr2017-proj3-group-2/lib/xgboost.R",
    "project_path" : "lib/xgboost.R",
    "properties" : {
    },
    "relative_order" : 9,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}