{
    "collab_server" : "",
    "contents" : "---\ntitle: 'Project 3: Main Script'\nauthor: \"Ka Heng (Helen) Lo\"\ndate: \"March 24, 2017\"\noutput:\n  pdf_document: default\n  html_document: default\n---\n\nThis file firstly details the model selection process for an advanced binary classification model, that takes in images and classifies them as either a labradoodle (\"1\") or fried chicken (\"0\") - simply referred to as the Advanced Model. Secondly, it runs evaluation experiments to compare the Baseline Model - gradient boosting model (gbm) with parameters tuned on the original SIFT features - with our Advanced Model.\n\n```{r} \n#Load (and install if necessary) required packages\nif(!require(\"EBImage\")){\n  source(\"https://bioconductor.org/biocLite.R\")\n  biocLite(\"EBImage\")\n}\n\n\nneeded <- setdiff(c(\"gbm\", \"data.table\", \"e1071\", \"class\", \"adabag\",\"caret\"),\n                  rownames(installed.packages()))\nif (length(needed) > 0 ){\n  install.packages(needed)\n}\nlibrary(EBImage)\nlibrary(gbm)\nlibrary(data.table)\nlibrary(e1071)\nlibrary(class)\nlibrary(adabag)\nlibrary(caret)\n```\n\n### Import class labels for training images  \nWe code labradoodles as \"1\" and fried chicken as \"0\" for binary classification.\n\n```{r training_images}\nlabel_train <- read.csv(\"../data/train/labels.csv\")\nlabel_train <- as.matrix(label_train)[,1]  \n#some of our models require labels to be numeric in [0,1], so we keep the labels as 0's and 1's\n#for any model that requires labels to be factors we pass the labels to the tune or train function\n#specific to the model as numeric values and coerce them inside the function to be factors\n```\n\n##Selecting the Advanced Model: \n\n### Model selection with provided SIFT features of training images  \nImport the SIFT features of the set of 2000 training images\n```{r}\ntrain_sift <- read.csv(\"../data/train/sift_features.csv\")\ntrain_sift <- t(as.matrix(train_sift))\n```\n\n#### Tune parameters of each model via cross-validation  \nSet up control for tuning models under consideration based on the provided SIFT features of the training images.  \n+ (T/F) tune models under consideration based on provided SIFT features  \n```{r}\n###### T/F to reproduce the tuning process for all models considered ######\nrun.tune.sift = FALSE\n```\n\n* Parameter-tuning process:  \n```{r}\n#get summary tables of best tuned parameters for each model considered, i.e. lowest cv error\n#6 columns: \"Model\", \"Best_Param_1\", \"Best_Param_2\", \"Best_Param_3\", \n#            \"Best_Error\", \"Training_Time\"\n#  - NA values for Best_Param_2 and/or Best_Param_3 if model has less than 3 params\n############################################################################################\n\n\n#source(\"../lib/xgboost.R\")\n#source(\"../lib/svm.R\")\n#source(\"../lib/knn.R\")\n#source(\"../lib/AdaBag_AdaBoost.R\")\n#source(\"../lib/blgbm.R\")\n\nif(run.tune.sift) {\n  source(\"../lib/xgboost.R\")\n  source(\"../lib/svm.R\")\n  source(\"../lib/knn.R\")\n  source(\"../lib/AdaBag_AdaBoost.R\")\n  source(\"../lib/blgbm.R\")\n  \n  summary.xgb <- tune.xgb(train_sift,label_train)\n  #save(summary.xgb, file=\"output/summary_best_xgb.RData\")\n\n  summary.svm.lin <- tune.svm.lin(train_sift,label_train)\n  #save(summary.svm.lin, file=\"output/summary_best_svm_lin.Rdata\")\n\n  summary.svm.rad <- tune.svm.rad(train_sift,label_train)\n  #save(summary.svm.rad,file=\"output/summary_best_svm_rad.Rdata\")\n\n  summary.knn <- tune.knn(train_sift,label_train)\n  #save(summary.knn, file=\"output/summary_best_knn.Rdata\")\n\n  summary.AdaBag <- tune.AdaBag(train_sift,label_train)\n  #save(summary.AdaBag,file=\"output/summary_best_AdaBag2.Rdata\")\n\n  summary.AdaBoost.M1 <- tune.AdaBoost.M1(train_sift,label_train)\n  #save(summary.AdaBoost.M1,file=\"output/summary_best_AdaBoost.M1.Rdata\")\n\n  summary.AdaBoost_SAMME <- tune.AdaBoost_SAMME(train_sift,label_train)\n  #save(summary.AdaBoost_SAMME,file=\"output/summary_best_AdaBoost_SAMME.Rdata\")\n  \n  summary.bl<- tune.bl(train_sift, label_train)\n  #save(summary.bl,file=\"output/summary_best_blgbm.RData\")\n}\n\n\n```\n\n#### Summary Table: parameter-tuning process on original SIFT features  \nSummary table of *best* models (i.e. models with *best* parameters) tuned on original SIFT features of the training set via cross-validation, sorted by CV Error.  \n```{r}\nlibrary(data.table)\n#load already produced summary tables for best tuned parameters for each model considered\nload(\"../output/summary_best_xgb.RData\")\nload(\"../output/summary_best_svm_lin.Rdata\")\nload(\"../output/summary_best_svm_rad.Rdata\")\nload(\"../output/summary_best_knn.Rdata\")\n\nload(\"../output/summary_best_AdaBag2.Rdata\") \n\nload(\"../output/summary_best_AdaBoost.M1.Rdata\")\nload(\"../output/summary_best_AdaBoost_SAMME.Rdata\")\n\nload(\"../output/summary_best_blgbm.Rdata\")\n\n#6 columns: \"Model\", \"Best_Param_1\", \"Best_Param_2\", \"Best_Param_3\", \n#            \"Best_Error\", \"Training_Time\"\n#  - NA values for Best_Param_2 and/or Best_Param_3 if model has less than 3 params\nsummary1 <- rbind(summary.xgb, summary.svm.lin,\n                 summary.svm.rad, \n                 summary.AdaBag2, summary.AdaBoost.M1,\n                 summary.AdaBoost_SAMME,\n                 summary.knn,summary.bl) \n\n#sort table by Best_Error in ascending order\nsummary1 <- summary1[order(summary1$Best_Error)]\n\n#add column for feature set models are tuned on\n(summary1 <- data.table(summary1[,1], Features = rep(\"Original SIFT\",8), summary1[,2:6]))\n\nsave(summary1,file=\"../output/summary_best_models1.Rdata\")\n\n```\n\n\n### Model selection with new visual features  \n#### Import sets of new visual features  \n```{r}\n#import the three csv files - the three new sets of visual features\nnew_train_feat1 <- read.csv(\"../data/train/sift_features_resize+adaptive.csv\")\nnew_train_feat1 <- t(as.matrix(new_train_feat1))\n\nnew_train_feat2 <- read.csv(\"../data/train/sift_features_resize.csv\")\nnew_train_feat2 <- t(as.matrix(new_train_feat2))\n\nnew_train_feat3 <- read.csv(\"../data/train/sift_features_adaptive.csv\")\nnew_train_feat3 <- t(as.matrix(new_train_feat3))\n```\n\n#### Tune parameters of each model via cross-validation  \n* Set up control for tuning models under consideration based on three new sets of features of the training images.  \n+ (T/F) tune models under consideration based on three new sets of visual features  \n```{r}\n###### T/F to reproduce the tuning process for all models considered ######\nrun.tune.new = FALSE\n```\n* Parameter-tuning process:  \n```{r}\nif (run.tune.new){\n  source(\"../lib/xgboost.R\")\n  source(\"../lib/blgbm.R\")\n  \n  ###******* Tune for xgBoost\n  #  1)tune on the first set of new visual features for the training images\n  summary.xgb.new1 <- tune.xgb(new_train_feat1,label_train)\n  #save(summary.xgb.new1,file=\"../output/summary_best_xgb_new1.Rdata\")\n  \n  #  2)tune on the second set of new visual features for the training images\n  summary.xgb.new2 <- tune.xgb(new_train_feat2,label_train)\n  #save(summary.xgb.new2,file=\"../output/summary_best_xgb_new2.Rdata\")\n  \n  #  3)tune on the third set of new visual features for the training images\n  summary.xgb.new3 <- tune.xgb(new_train_feat3,label_train)\n  #save(summary.xgb.new3,file=\"../output/summary_best_xgb_new3.Rdata\")\n  \n  \n  ###******* Tune for gbm \n  #  1)tune on the first set of new visual features for the training images\n  summary.gbm.new1 <- bl.tune(new_train_feat1,label_train)\n  #save(summary.gbm.new1,file=\"../output/summary_best_gbm_new1.Rdata\")\n\n  #  2)tune on the second set of new visual features for the training images\n  summary.gbm.new2 <- bl.tune(new_train_feat2,label_train)\n  #save(summary.gbm.new2,file=\"../output/summary_best_gbm_new2.Rdata\")\n  \n  #  3)tune on the third set of new visual features for the training images\n  summary.gbm.new3 <- bl.tune(new_train_feat3,label_train)\n  #save(summary.gbm.new3,file=\"../output/summary_best_gbm_new3.Rdata\")\n  \n}\n```\n\n\n#### Summary Table: parameter-tuning process on new features  \n```{r}\n#load Rdata files\nload(\"../output/summary_best_xgb_new1.Rdata\")\nload(\"../output/summary_best_xgb_new2.Rdata\")\nload(\"../output/summary_best_xgb_new3.Rdata\")\nload(\"../output/summary_best_gbm_new1.Rdata\")\nload(\"../output/summary_best_gbm_new2.Rdata\")\nload(\"../output/summary_best_gbm_new3.Rdata\")\n\nsummary2 <- rbind(summary.xgb.new1, summary.xgb.new2,\n                 summary.xgb.new3, summary.gbm.new1,\n                 summary.gbm.new2, summary.gbm.new3) \n#sort table by Best_Error in ascending order\nsummary2 <- summary2[order(summary2$Best_Error)]\n\n#add column for feature set models are tuned on\n(summary2 <- data.table(summary2[,1], Features = c(\"SIFT- resize+adaptive\",\"SIFT- resize\",\n                                                     \"SIFT- adaptive\", \"SIFT- resize+adaptive\",\n                                                     \"SIFT- resize\", \"SIFT- adaptive\"),\n                        summary2[,2:6]))\n\nsave(summary2,file=\"../output/summary_best_models2.Rdata\")\n```\n  \n### Convolutional Neural Network (CNN)  \n#### Tune CNN model on set of raw training images via cross-validation  \n* Set up control for tuning CNN on set of raw images  \n+ (T/F) tune CNN  \n```{r}\n###### T/F to reproduce the tuning process for all models considered ######\nrun.tune.cnn = FALSE\n```\n* Parameter-tuning process  \n```{r}\nif (run.tune.cnn) {\n  img_train_dir <- \"../data/train/raw_images\"\n  source(\"../lib/CNN.R\")\n  source(\"../lib/CNNcv.R\")\n  \n  #first resize the raw images and then split into test set(200) and train set(1800)\n  data_list <- to.resize.split(img_train_dir,label_train)\n  \n  \n  #cross-validation on the resized train set (1800 images)\n  iter <- seq(60,100,by=10) #iter is the range of the tune parameters\n  #get best num.rounds\n  best_num.rounds <- CNNcv(data_list$train_data,iter)\n  \n  #retrain data (1800 images) ; get run.time; and test error using test set(200 images)\n  cnn.output <- CNN(data_list$train_data,data_list$test_data)\n  \n  summary.cnn <- data.table(Model = \"CNN\", Features = NA,\n             Best_Param_1 = paste(\"num.rounds =\",best_num.rounds),\n             Best_Param_2 = NA,\n             Best_Param_3 = NA,\n             Best_Error = cnn.output$test_err,\n             Training_Time = paste(cnn.output$train_time, \"s\"))\n  #save(summary.cnn, file=\"output/summary_best_cnn.RData\")\n\n}\n```\n\n  \n### Selecting the *best* Advanced Model  \n#### Summary Table of best models tuned on various feature sets (original SIFT & new) + CNN   \n```{r}\n#load Rdata files \nload(\"../output/summary_best_models1.Rdata\")\nload(\"../output/summary_best_models2.Rdata\")\n\nload(\"../output/summary_best_cnn.Rdata\")\n\nsummary3 <- rbind(summary1,summary2,summary.cnn)\n(summary3 <- summary3[order(summary3$Best_Error)])\n\nsave(summary3,file=\"../output/summary_best_models3.Rdata\")\nsave(summary3,file=\"../output/summary_best_models3.Rdata\")\n```\n  \nIn the summary table, the models under consideration are ranked by best (min) error. The Baseline Model (GBM with original SIFT) is ranked 7th with cross-validation estimated prediction error of .2365 and training time of 15.234 seconds. Model 1 in the table (XGBoost with original SIFT) has the lowest estimated prediction error of 5.267342e-05 (<<.2365), but it has a higher training time of 49.148 seconds (>15.234 seconds). Model 2 in the table (CNN with 1800 of the 2000 raw images) has an test error (testing with the remaining 200 of the 2000 raw images) of .1355, but it has a really large training time of 321.5955 seconds (>>15.234 seconds). Model 3 in the table (XGBoost with SIFT- resize+adaptive) has a decently low cross-validation estimated prediction error of .189 (<.2365) and a decently low training time of 19.032 seconds (only slightly larger, but comparable to 15.234 seconds). Thus, we chose Model 3 (XGBoost with SIFT- resize+adaptive) to be our advanced model, as it seems the best compromise between a low estimated prediction error and a small training time.  \n  \n\n## Comparing the Baseline Model and the Advanced Model  \n### Set up controls for evaluation experiments: Baseline Model vs. Advanced Model\n\n+ (T/F) train Baseline Model and Advanced Model\n+ (T/F) process features for training set\n+ (T/F) run evaluation on an independent test set for both Baseline Model and Advanced Model\n+ (T/F) process features for test set\n\n```{r exp_setup}\nrun.train=TRUE # train 'best' model \nrun.feature.train=TRUE # process features for training set\nrun.test=FALSE # run evaluation on an independent test set\nrun.feature.test=FALSE # process features for test set\n```\n\n### Import given SIFT features new visual features of the test set  \nWe construct the new visual features outside of R/Rstudio, so we just import the .csv files with the new feature data.\n```{r}\n#Recall for the training set we imported the .csv files before\nif (run.feature.train){\n  #dat_train_sift <- read.csv(\"../data/train/sift_features.csv\")\n  #dat_train_sift <- t(as.matrix(dat_train_sift))\n  dat_train_sift <- train_sift\n  save(dat_train_sift,file=\"../output/dat_train_sift.Rdata\")\n  \n  #dat_train_new <- read.csv(\"../data/train/sift_features_resize+adaptive.csv\")\n  #dat_train_new <- t(as.matrix(dat_train_new))\n  dat_train_new <- new_train_feat1 #we selected new feature set 1: SIFT- resize+adaptive\n  save(dat_train_new,file=\"../output/dat_train_new.Rdata\")\n}\n\n##Now for the test set: \nif (run.feature.test){\n  #Import the provide SIFT features of the test set\n  dat_test_sift <- read.csv(\"../data/test/sift_features_test.csv\")\n  dat_test_sift <- t(as.matrix(dat_test_sift))\n  save(dat_test_sift,file=\"../output/dat_test_sift.Rdata\")\n  #Import the set of new visual features (constructed outside of R)\n  dat_test_new <- read.csv(\"../data/test/sift_features_resize+adaptive.csv\")  # ***or other file name\n  dat_test_new <- t(as.matrix(dat_test_new))\n  save(dat_test_new,file=\"../output/dat_test_new.Rdata\")\n}\n\n#tm_feature_test = NA\n```\n\n### Train a classification model with training images\n`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. \n+ `train.R`\n  + Input: an R object that contains processed training set features.\n  + Input: an R object of training sample labels.\n  + Output: an RData file that contains trained classifiers in the forms of R objects: models/settings/links to external trained configurations.\n+ `test.R`\n  + Input: a path that points to the test set features.\n  + Input: an R object that contains a trained classifiers.\n  + Output: an R object of class label predictions on the test set. If there are multiple classifiers under evaluation, there should be multiple sets of label predictions. \n```{r loadlib}\nsource(\"../lib/train.R\")\nsource(\"../lib/test.R\")\n```\n\n\n### Train Baseline Model and Advanced Model  \nHere we call the ```train()``` function in ```train.R```. We fit the entire set of constructed visual features of the training images to the Baseline Model and the Advanced Model.  \n```{r final_train}\n#Here, we call train() from train.R\n#   -note that train() returns a list of two model objects: list(baseline_fit,advanced_fit)\ntm_train=NA ; tm_train_base=NA ; tm_train_adv=NA\nif(run.train){\n  load(file=\"../output/dat_train_sift.Rdata\")\n  load(file=\"../output/dat_train_new.Rdata\")\n  tm_train <- system.time(fit_train <- train(dat_train_base = dat_train_sift, \n                                             dat_train_adv = dat_train_new, \n                                             label_train=label_train))\n  save(fit_train, file=\"../output/fit_train.Rdata\")\n  \n  #individual times for training base model and advanced model \n  tm_train_base <- system.time(fit_train_base <- train(dat_train_base = dat_train_sift,\n                                                       label_train=label_train, model=\"base\"))\n  tm_train_adv <- system.time(fit_train_adv <- train(dat_train_adv = dat_train_new,\n                                                     label_train=label_train,model=\"advanced\"))\n}\n\n\n```\n\n### Make predictions  \nHere we call the ```test()``` function in ```test.R```. Feed the Baseline Model and the Advanced Model with the completely holdout testing data.  \n```{r test}\n#################################################################################################\n#to test the test() function, get predictions on training data for base model and advanced model\nload(file=\"../output/fit_train.Rdata\")\nload(file=\"../output/dat_train_sift.Rdata\")\nload(file=\"../output/dat_train_new.Rdata\")\n(pred_train <- test(fit_train = fit_train, dat_test_base = dat_train_sift,\n                   dat_test_adv = dat_train_new))\n#################################################################################################\n#Make predictions and record the time it takes \ntm_test=NA\nif(run.test){\n  load(file=\"../output/dat_test_sift.Rdata\")\n  load(file=\"../output/dat_test_new.Rdata\")\n  load(file=\"../output/fit_train.Rdata\")\n  tm_test <- system.time(pred_test <- test(fit_train = fit_train, \n                                           dat_test_base = dat_test_sift,\n                                           dat_test_adv = dat_test_new))\n  save(pred_test, file=\"../output/pred_test.RData\")\n  \n  #individual times for making predictions using base model and advanced model\n  tm_test_base <- system.time(pred_test_base <- test(fit_train = fit_train_base, \n                                                     dat_test_base = dat_test_sift,\n                                                     model=\"base\"))\n  tm_test_adv <- system.time(pred_test_adv <- test(fit_train = fit_train_adv, \n                                                   dat_test_adv = dat_test_new, \n                                                   model=\"advanced\"))\n}\n\n```\n\n### Summarize Running Time\nPrediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. \n```{r running_time}\nif(run.feature.train){\n  load(\"../output/tm_new_feature_train.Rdata\")   #tm_new_feature_train\n  cat(\"Time for constructing new training features=\", tm_new_feature_train[1], \"s \\n\")\n}\nif (run.feature.test){\nload(\"../output/tm_new_feature_test.Rdata\")   #tm_new_feature_test\ncat(\"Time for constructing new testing features=\", tm_new_feature_test[1], \"s \\n\")\n}\nif (run.train){\ncat(\"Time for training models (Baseline + Advanced)=\", tm_train[1], \"s \\n\")\ncat(\"Time for training Baseline Model=\", tm_train_base[1], \"s \\n\")\ncat(\"Time for training Advanced Model=\", tm_train_adv[1], \"s \\n\")\n}\nif(run.test){\ncat(\"Time for making predictions (Baseline + Advanced)=\", tm_test[1], \"s \\n\")\ncat(\"Time for making predictions with Baseline Model=\", tm_test_base[1], \"s \\n\")\ncat(\"Time for making predictions with Advanced Model=\", tm_test_adv[1], \"s \\n\")\n}\n```\n",
    "created" : 1489510301902.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4245564006",
    "id" : "A952F246",
    "lastKnownWriteTime" : 1490326259,
    "last_content_update" : 1490326259159,
    "path" : "~/Desktop/Applied Data Science/Proj3/spr2017-proj3-group-2/doc/main.Rmd",
    "project_path" : "doc/main.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}