{
    "collab_server" : "",
    "contents" : "#####################cross validate the best number of iterations#####################\n \n#iter <- seq(60,100,by=10)\n#iter is the range of the tune parameters\nCNNcv <- function(train,iter){\n  errorvec <- matrix(NA,nrow = length(iter),ncol = 2)\n  index <- sample(rep(1:5,each = nrow(train)/5),replace = F)\n  \n  #set the value for the parameters: symbol (=NN_model) and ctx (=devices)\n  #to be passed to mx.model.FeedForward.create() later\n  data <- mx.symbol.Variable('data')\n  # 1st convolutional layer\n  conv_1 <- mx.symbol.Convolution(data = data, kernel = c(5, 5), num_filter = 20)\n  tanh_1 <- mx.symbol.Activation(data = conv_1, act_type = \"tanh\")\n  pool_1 <- mx.symbol.Pooling(data = tanh_1, pool_type = \"max\", kernel = c(2, 2), stride = c(2, 2))\n  # 2nd convolutional layer\n  conv_2 <- mx.symbol.Convolution(data = pool_1, kernel = c(5, 5), num_filter = 50)\n  tanh_2 <- mx.symbol.Activation(data = conv_2, act_type = \"tanh\")\n  pool_2 <- mx.symbol.Pooling(data=tanh_2, pool_type = \"max\", kernel = c(2, 2), stride = c(2, 2))\n  # 1st fully connected layer\n  flatten <- mx.symbol.Flatten(data = pool_2)\n  fc_1 <- mx.symbol.FullyConnected(data = flatten, num_hidden = 500)\n  tanh_3 <- mx.symbol.Activation(data = fc_1, act_type = \"tanh\")\n  # 2nd fully connected layer\n  fc_2 <- mx.symbol.FullyConnected(data = tanh_3, num_hidden = 40)\n  # Output. Softmax output since we'd like to get some probabilities.\n  NN_model <- mx.symbol.SoftmaxOutput(data = fc_2)\n  \n  # Set seed for reproducibility\n  mx.set.seed(100)\n  devices <- mx.cpu()\n  \n  j <- 1\n  for(i in iter){\n    error <- c()\n    for(k in 1:5){\n      subtrain <- (data.matrix(train))[index!=k,]\n      train_x1 <- t(subtrain[, -1])\n      train_y1 <- subtrain[, 1]\n      train_array1 <- train_x1\n      dim(train_array1) <- c(28, 28, 1, ncol(train_x1))\n\n      subcv <- (data.matrix(train))[index==k,]\n      test_x1 <- t(subcv[, -1])\n      test_y1 <- subcv[, 1]\n      test_array1 <- test_x1\n      dim(test_array1) <- c(28, 28, 1, ncol(test_x1))\n\n      model <- mx.model.FeedForward.create(NN_model,\n                                           X = train_array1,\n                                           y = train_y1,\n                                           ctx = devices,\n                                           num.round =i,\n                                           array.batch.size = 40,\n                                           learning.rate = 0.01,\n                                           momentum = 0.9,\n                                           eval.metric = mx.metric.accuracy,\n                                           epoch.end.callback = mx.callback.log.train.metric(100))\n      predicted <- predict(model, test_array1)\n      # Assign labels\n      predicted_labels <- max.col(t(predicted)) - 1\n      # Get accuracy\n      error[k] <- sum(diag(table(test_y1, predicted_labels)))/length(test_y1)\n    }\n    errorvec[j,1] <- i\n    errorvec[j,2] <- mean(error)\n    j <- j+1\n  }\n  para<- errorvec[which.max(errorvec[,2]),1]\n  cat(\"The best num.rounds is:\", para)\n  return(para)\n}\n#######################################################",
    "created" : 1490310718309.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2357589479",
    "id" : "7AB5530D",
    "lastKnownWriteTime" : 1490314313,
    "last_content_update" : 1490314313952,
    "path" : "~/Desktop/Applied Data Science/Proj3/spr2017-proj3-group-2/lib/CNNcv.R",
    "project_path" : "lib/CNNcv.R",
    "properties" : {
    },
    "relative_order" : 8,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}